{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bihji0AF8Lf0"
      },
      "source": [
        "#Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8hdrKiyRKas"
      },
      "source": [
        "## Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cheAujcY8Bzx"
      },
      "outputs": [],
      "source": [
        "pip install gmplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hri3-SevjsBD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdcFicCb8oNM"
      },
      "outputs": [],
      "source": [
        "pip install geopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an6zxOc08qGa"
      },
      "outputs": [],
      "source": [
        "#Constant\n",
        "api_key = \"AIzaSyAlL7KTo7q7UXYyPtJzjqLtqgKk9UjP4bY\" #Google API key \n",
        "\n",
        "#Courier company dictionary\n",
        "## in case asking changing input, change latutude of City-link to 3.0359924887507144\n",
        "courier_dict = {\n",
        "    0: {\n",
        "        \"name\": \"City-link Express\",\n",
        "        \"location\": \"Port Klang\",\n",
        "        \"latitude\": 3.0319924887507144,\n",
        "        \"longitude\": 101.37344116244806\n",
        "    },\n",
        "    1: {\n",
        "        \"name\": \"Pos Laju\",\n",
        "        \"location\": \"Petaling Jaya\",\n",
        "        \"latitude\": 3.112924170027219,\n",
        "        \"longitude\": 101.63982650389863\n",
        "    },\n",
        "    2: {\n",
        "        \"name\": \"GDEX\",\n",
        "        \"location\": \"Batu Caves\",\n",
        "        \"latitude\": 3.265154613796736,\n",
        "        \"longitude\": 101.68024844550233\n",
        "    },\n",
        "    3: {\n",
        "        \"name\": \"J&T\",\n",
        "        \"location\": \"Kajang\",\n",
        "        \"latitude\": 2.9441205329488325,\n",
        "        \"longitude\": 101.7901521759029\n",
        "    },\n",
        "    4: {\n",
        "        \"name\": \"DHL\",\n",
        "        \"location\": \"Sungai Buloh\",\n",
        "        \"latitude\": 3.2127230893650065,\n",
        "        \"longitude\": 101.57467295692778\n",
        "    }\n",
        "}\n",
        "\n",
        "#Customer Origin dictionary\n",
        "customer_ori_dict= { \n",
        "    1:{\n",
        "       \"name\": \"Customer 1\",\n",
        "       \"location\": \"Rawang\",\n",
        "       \"latitude\" :3.3615395462207878,\n",
        "       \"longitude\" :101.56318183511695,\n",
        "    },\n",
        "    2:{\n",
        "       \"name\": \"Customer 2\",\n",
        "       \"location\": \"Subang Jaya\",\n",
        "       \"latitude\" :3.049398375759954,\n",
        "       \"longitude\" :101.58546611160301,\n",
        "    },\n",
        "    3:{\n",
        "       \"name\": \"Customer 3\",\n",
        "       \"location\": \"Ampang\",\n",
        "       \"latitude\" :3.141855957281073,\n",
        "       \"longitude\" :101.76158583424586,\n",
        "    }\n",
        "}\n",
        "\n",
        "#Customer Destination dictionary\n",
        "customer_dest_dict= { \n",
        "    1:{\n",
        "       \"name\": \"Customer 1\",\n",
        "       \"location\": \"Bukit Jelutong\",\n",
        "       \"latitude\" :3.1000170516638885,\n",
        "       \"longitude\" : 101.53071480907951\n",
        "    },\n",
        "    2:{\n",
        "       \"name\": \"Customer 2\",\n",
        "       \"location\" : \"Puncak Alam\",\n",
        "       \"latitude\" :3.227994355250716,\n",
        "       \"longitude\" :101.42730357605375\n",
        "    },\n",
        "    3:{\n",
        "       \"name\": \"Customer 3\",\n",
        "       \"location\" : \"Cyberjaya\",\n",
        "       \"latitude\" :2.9188704151716256,\n",
        "       \"longitude\" : 101.65251821655471\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVgEWtwQRN2-"
      },
      "source": [
        "##Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBDWGN5uZNmC"
      },
      "outputs": [],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1nrfSNuM7eEYvRjmHSENWxvlS1CYe07zg\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Customer 1 (ev).html') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1vNAXPuOv4drXBhiTTjVYmveOsrcFzL1W\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Customer 2 (ev).html') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1BrRULVgz5WNKo2p2e_ngUT8WL0_rbuib\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Customer 3 (ev).html') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaLpqnSRSiP"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cs2lw8V80V6"
      },
      "outputs": [],
      "source": [
        "#Algorithm \n",
        "\n",
        "from geopy.distance import geodesic\n",
        "import heapq\n",
        "\n",
        "# Simple weighted graph implementaion\n",
        "class WeightedGraph:\n",
        "  def __init__(self):\n",
        "    self.edges = {}\n",
        "\n",
        "  # Get the cost (edge value) between two node\n",
        "  def cost(self, from_node, to_node):\n",
        "    possible_destination = self.edges.get(from_node)\n",
        "    return possible_destination.get(to_node, 1)\n",
        "\n",
        "  # Get the neighbour value from \n",
        "  def neighbours(self, x):\n",
        "      return self.edges[x].keys();\n",
        "\n",
        "class PriorityQueue:\n",
        "  def __init__(self):\n",
        "    self.elements = []\n",
        "  \n",
        "  def empty(self):\n",
        "    return not self.elements\n",
        "\n",
        "  def put(self, item, priority):\n",
        "    heapq.heappush(self.elements, (priority, item))\n",
        "\n",
        "  def get(self):\n",
        "    return heapq.heappop(self.elements)[1]\n",
        "\n",
        "# This function construct the path (Start to Goal)\n",
        "def reconstruct_path(came_from, start, goal):\n",
        "  current = goal\n",
        "  path = []\n",
        "  while current != start:\n",
        "    path.append(current)\n",
        "    current = came_from[current]\n",
        "  path.append(start)\n",
        "  path.reverse()\n",
        "  return path\n",
        "\n",
        "# Get the latitude and logitude value for a particular location.\n",
        "def get_position(location, counter, courier_dict,customer_ori_dict,customer_dest_dict):\n",
        "  if location == \"Origin\":\n",
        "    return (customer_ori_dict[counter][\"latitude\"], customer_ori_dict[counter][\"longitude\"])\n",
        "  elif location == \"Destination\":\n",
        "    return (customer_dest_dict[counter][\"latitude\"], customer_dest_dict[counter][\"longitude\"])\n",
        "  else:\n",
        "    return (courier_dict[location][\"latitude\"], courier_dict[location][\"longitude\"])\n",
        "      \n",
        "\n",
        "# Calculate the heuristic value between two location. \n",
        "# Note: The heuristic estimated value must be less that the actual distance to ensure getting the optimal path.\n",
        "#       In this case we use geodesic to compute the heuristic value.\n",
        "def heuristic(loc_a, loc_b,counter,courier_dict, customer_ori_dict,customer_dest_dict):\n",
        "  geo_pos_a = get_position(loc_a,counter,courier_dict,  customer_ori_dict,customer_dest_dict)\n",
        "  geo_pos_b = get_position(loc_b,counter,courier_dict, customer_ori_dict,customer_dest_dict)\n",
        "  return geodesic(geo_pos_a, geo_pos_b).km\n",
        "\n",
        "\n",
        "# Function that find the optimal path using heuristic value.\n",
        "def a_star_search(graph, start, goal,counter,courier_dict, customer_ori_dict,customer_dest_dict):\n",
        "\n",
        "  # Initialise a priority queue.\n",
        "  node_pq = PriorityQueue()\n",
        "\n",
        "  # Insert the start node with the priority value (added with heuristic) to the priority queue.\n",
        "  node_pq.put(start, 0)\n",
        "\n",
        "  # Initialise an empty dictionary to store where the node is come from.\n",
        "  came_from = {}\n",
        "\n",
        "  # Initalise an empty dictonary to store the travel cost (Only involved actual value).\n",
        "  cost_so_far = {}\n",
        "  # Insert the start node.\n",
        "  came_from[start] = None\n",
        "  cost_so_far[start] = 0\n",
        "\n",
        "  while not node_pq.empty():\n",
        "\n",
        "    # Obtain the element with the lowest cost value.\n",
        "    current = node_pq.get()\n",
        "\n",
        "    # Break out of the loop if the current_node = goal_node\n",
        "    if current == goal:\n",
        "      break\n",
        "\n",
        "    # Looping all the neighbour nodes.\n",
        "    for next in graph.neighbours(current):\n",
        "\n",
        "      # Calculate the actual current cost.\n",
        "      new_cost = cost_so_far[current] + graph.cost(current, next)\n",
        "\n",
        "      # Check whether the node inside the dictionary or the new cost is smaller than the next node that is available inside the cost_so_far dictionary.\n",
        "      if next not in cost_so_far or new_cost < cost_so_far[next]:\n",
        "\n",
        "        # Insert the new_cost with the 'next' element (haven't travel) into the dictionary.\n",
        "        cost_so_far[next] = new_cost\n",
        "\n",
        "        # Calculate the priority using f(n) = g(n) + h(n).\n",
        "        priority = new_cost + heuristic(next, goal,counter, courier_dict, customer_ori_dict,customer_dest_dict)\n",
        "        node_pq.put(next, priority)\n",
        "        came_from[next] = current\n",
        "\n",
        "  return came_from, cost_so_far"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNnPGddsRRda"
      },
      "source": [
        "## Functions Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq-PIWBf8wWa"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "import gmplot\n",
        "\n",
        "def getAddress(place_dict):\n",
        "  \"\"\"\n",
        "  This function compute actual address by obtainning the coordinate from the dictionary\n",
        "  It print the address in format as such:\n",
        "  {Name} ({Location}) : {Address}\n",
        "  Sample output: \n",
        "  City-link Express (Port Klang): \n",
        "  Address: Bandar Sultan Suleiman, Majlis Perbandaran Klang, Klang, Selangor, 42009, Malaysia\n",
        "  \"\"\"\n",
        "  geolocator = Nominatim(user_agent=\"Geo\")\n",
        "  locator_list = []\n",
        "\n",
        "  #reverse to get the address by using Nominatim\n",
        "  for item in place_dict.values():\n",
        "    locator_list.append([item[\"name\"], item['location'] , geolocator.reverse((item[\"latitude\"], item[\"longitude\"]))])\n",
        "\n",
        "  #display address\n",
        "  for name,location, geoloc in locator_list:\n",
        "    print(f\"{name} ({location}): \\nAddress: {geoloc.address}\\n\")\n",
        "\n",
        "\n",
        "def plotLocation(courier_dict, output_name):\n",
        "  \"\"\"\n",
        "  This function plot location by obtainning the coordinate from the dictionary\n",
        "  It yield map in form of html and rename to output_name.html\n",
        "  \"\"\"\n",
        "  api_key =  \"AIzaSyAlL7KTo7q7UXYyPtJzjqLtqgKk9UjP4bY\"\n",
        "  gmap = gmplot.GoogleMapPlotter(3.0, 101.4, 9,apikey=api_key)\n",
        "  \n",
        "  #Extract coordinate from the courier_dict\n",
        "  coordinate = [(item['latitude'],item['longitude']) for item in courier_dict.values()]\n",
        "\n",
        "  #scatter plot on coordinate\n",
        "  gmap.scatter(*zip(*coordinate),\n",
        "              color = ['red', 'orange', 'yellow', 'green', 'blue'], \n",
        "              size = 20,\n",
        "              title = [item['name'] for item in courier_dict.values()],\n",
        "              marker = True,\n",
        "              label=['A', 'B', 'C', 'D', 'E'])\n",
        "  \n",
        "  #draw map in form of html\n",
        "  gmap.draw(output_name+'.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQvkRjfRRn51"
      },
      "source": [
        "##Functions Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDAdT36q86Z1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import itertools\n",
        "import numpy as np\n",
        "hub_totalDistance_list = [] #storing for usage later in problem 3\n",
        "\n",
        "def getDistance_preprocess(coor_lst):\n",
        "  #Propress into string format according to url parameter format \n",
        "  \"\"\"\n",
        "  Sample output:\n",
        "  x,y   (no space between 'x' ',' and 'y')\n",
        "  \"\"\"\n",
        "  return \",\".join(f\"{x_y}\" for x_y in coor_lst)\n",
        "\n",
        "#get distance from origin to destination of each customer\n",
        "def getDistance(name,origin,destination): \n",
        "  apiKey =  \"AIzaSyAlL7KTo7q7UXYyPtJzjqLtqgKk9UjP4bY\"\n",
        "  #json format\n",
        "  #default is driving mode\n",
        "  ori =getDistance_preprocess(origin)\n",
        "  dest =getDistance_preprocess(destination)\n",
        "  #API URL for request\n",
        "  request_url = f\"https://maps.googleapis.com/maps/api/distancematrix/json?origins={ori}&destinations={dest}&key={apiKey}\"\n",
        "  reply_dict= requests.get(request_url).json()\n",
        "  \n",
        "  #retrieve info based on returned json format and arrange in form of usage format\n",
        "  info_list = [[name,\n",
        "                reply_dict[\"origin_addresses\"][0], \n",
        "                reply_dict['destination_addresses'][0], \n",
        "                reply_dict['rows'][0]['elements'][0][\"distance\"][\"value\"]]]\n",
        "\n",
        "  return info_list\n",
        "\n",
        "\n",
        "def findShortestPath(hub_firstPath_list, hub_secondPath_list ,courier_dict, customer_ori_dict,customer_dest_dict):\n",
        "  customer_graph_list = [WeightedGraph(),WeightedGraph(),WeightedGraph()]\n",
        "  shortest_path_dict = {}\n",
        "  k=0 #counter to locate the desired element by index\n",
        "\n",
        "  for c_graph in customer_graph_list:\n",
        "    c_graph.edges = {\"Origin\":{j: hub_firstPath_list[k][j] for j in range(len(hub_firstPath_list[k]))}}\n",
        "\n",
        "    for j in range(len(hub_secondPath_list[k])):\n",
        "      c_graph.edges[j] = {'Destination': hub_secondPath_list[k][j]}\n",
        "    c_graph.edges['Destination'] = {}\n",
        "    x, y = a_star_search(c_graph, 'Origin', 'Destination', (k+1) , courier_dict, customer_ori_dict,customer_dest_dict)\n",
        "    shortest_path = reconstruct_path(x, 'Origin', 'Destination')\n",
        "    shortest_path_dict[k]= shortest_path\n",
        "    k+=1\n",
        "  #return list of 3 element which show the index of courier having shortest distance\n",
        "  return [item[1] for item in shortest_path_dict.values()]\n",
        "\n",
        "def getPairedList(ori_dict,dest_dict,isHub):\n",
        "  \"\"\"\n",
        "  Pair up origin and destination information and return to be used later\n",
        "  \"\"\"\n",
        "\n",
        "  if isHub is False:\n",
        "    name_list = [item['name'] for item in ori_dict.values() ]\n",
        "    ori_list = [[item[\"latitude\"],item[\"longitude\"]] for item in ori_dict.values()]\n",
        "    dest_list = [[item[\"latitude\"],item[\"longitude\"]] for item in dest_dict.values()]\n",
        "    return list(zip(name_list,ori_list, dest_list))\n",
        "\n",
        "  elif len(ori_dict)>len(dest_dict):\n",
        "    size = len(ori_dict)\n",
        "    ori_list = [[item[\"latitude\"],item[\"longitude\"]] for item in ori_dict.values()]\n",
        "    dest_list = [[item[\"latitude\"],item[\"longitude\"]] for item in dest_dict.values()]*size\n",
        "\n",
        "  else:\n",
        "    size = len(dest_dict)\n",
        "    ori_list = [[item[\"latitude\"],item[\"longitude\"]] for item in ori_dict.values()]*size\n",
        "    dest_list = [[item[\"latitude\"],item[\"longitude\"]] for item in dest_dict.values()]\n",
        "\n",
        "  return list(zip(ori_list, dest_list))\n",
        "\n",
        "def getPathDistance(pathList,i ):\n",
        "  \"\"\"\n",
        "  Obtain path distance in kilometer\n",
        "  \"\"\"\n",
        "  hub_Path= []\n",
        "  for paired_tuple in pathList:\n",
        "      tempList = getDistance(f\"Customer {i}\",paired_tuple[0],paired_tuple[1])\n",
        "      tempList = itertools.chain(*tempList)\n",
        "      hub_Path.append(list(tempList)[-1]/1000.00)\n",
        "  return hub_Path\n",
        "\n",
        "def plotHubs(courier_dict,gmap):\n",
        "  \"\"\"\n",
        "  This function plot location by obtainning the coordinate from the dictionary\n",
        "  \"\"\"  \n",
        "  #Extract coordinate from the courier_dict\n",
        "  coordinate = [(item['latitude'],item['longitude']) for item in courier_dict.values()]\n",
        "\n",
        "  #Colour for the hub label\n",
        "  colour = ['red', 'orange', 'yellow', 'green', 'blue']\n",
        "\n",
        "  #title of the marker\n",
        "  titles = [item['name'] for item in courier_dict.values()]\n",
        "\n",
        "  #info window settings in format of {Name} Hub: {Location} (latitude, longitude)\n",
        "  info_windows = [f\"{item['name']} Hub: {item['location']} ({item['latitude']},{item['longitude']})\"for item in courier_dict.values()]\n",
        "\n",
        "  #Mark all point\n",
        "  for i in range(len(coordinate)):\n",
        "    gmap.marker(*coordinate[i], color = colour[i], title = titles[i] , label = 'H' ,info_window = info_windows[i])\n",
        " \n",
        "\n",
        "def plotMap(courier_dict,customer_ori_dict,customer_dest_dict, output_name,i, shortestPathIndex):\n",
        "  \"\"\"\n",
        "  This function plot the full Map of each customer \n",
        "  \"\"\"\n",
        "  api_key =  \"AIzaSyAlL7KTo7q7UXYyPtJzjqLtqgKk9UjP4bY\"\n",
        "  colour = ['red', 'orange', 'yellow', 'green', 'blue']\n",
        "  gmap = gmplot.GoogleMapPlotter(3.0, 101.4, 9,title = output_name,apikey=api_key)\n",
        "   \n",
        "  #Extract coordinate from the courier_dict\n",
        "  customer_ori_coor = (customer_ori_dict[i]['latitude'], customer_ori_dict[i]['longitude'])\n",
        "  customer_dest_coor = (customer_dest_dict[i]['latitude'], customer_dest_dict[i]['longitude'])\n",
        "  customer_coor = [customer_ori_coor , customer_dest_coor]\n",
        "\n",
        "  gmap.directions(customer_ori_coor , customer_dest_coor)\n",
        "  #scatter plot on coordinate\n",
        "  plotHubs(courier_dict, gmap)\n",
        "  for i in range(len(courier_dict)):\n",
        "    hub_coor = (courier_dict[i]['latitude'], courier_dict[i]['longitude'])\n",
        "    gmap.directions(customer_ori_coor, customer_dest_coor, color = colour[i],\n",
        "                   waypoints = [hub_coor])\n",
        "  #plotHubs(courier_dict, gmap)\n",
        "  gmap.draw(f'{output_name}.html')\n",
        "\n",
        "def analyseShortestDistance(customer_ori_dict, customer_dest_dict,courier_dict):\n",
        "  actualDistance_info_list = []\n",
        "  customer_pair_list = getPairedList(customer_ori_dict,customer_dest_dict,False)\n",
        "  for paired_tuple in customer_pair_list:\n",
        "    actualDistance_info_list.append(getDistance(paired_tuple[0],paired_tuple[1],paired_tuple[2]))\n",
        "\n",
        "  hub_firstPath_list = []\n",
        "  hub_secondPath_list = []\n",
        "\n",
        "  for i in range(1,len(customer_ori_dict)+1):\n",
        "    #Slicing of dictionary to obtain customer i \n",
        "    chosenCustomer_ori_dict = dict(list(customer_ori_dict.items())[i-1:i])\n",
        "    chosenCustomer_dest_dict = dict(list(customer_dest_dict.items())[i-1:i])\n",
        "    \n",
        "    #Pair up dictionary and change it to list for usage of looping\n",
        "    firstPath_pair_list = getPairedList(chosenCustomer_ori_dict,courier_dict,True)\n",
        "    #add distance list of customer i for first path\n",
        "    hub_firstPath_list.append(getPathDistance(firstPath_pair_list,i))\n",
        "    \n",
        "    #Pair up dictionary and change it to list for usage of looping\n",
        "    secondPath_pair_list = getPairedList(courier_dict,chosenCustomer_dest_dict,True)\n",
        "    #add distance list of customer i for second path\n",
        "    hub_secondPath_list.append(getPathDistance(secondPath_pair_list,i))\n",
        "\n",
        "  #Round off to 3.d.p, Distance all are in kilometre\n",
        "  global hub_totalDistance_list\n",
        "  hub_totalDistance_list =np.round(np.add(np.array(hub_firstPath_list),np.array(hub_secondPath_list)),3).tolist()\n",
        "\n",
        "  #flatten the list for 1 dimension\n",
        "  actualDistance_info_list = itertools.chain(*actualDistance_info_list)\n",
        "\n",
        "  #find shortest path by A* Search\n",
        "  shortestPath_lst = findShortestPath(hub_firstPath_list, hub_secondPath_list ,courier_dict, customer_ori_dict,customer_dest_dict)\n",
        "\n",
        "  #Plot all route in html format\n",
        "  #Do note that \n",
        "  for i in range(len(shortestPath_lst)):\n",
        "    plotMap(courier_dict, customer_ori_dict , customer_dest_dict ,f\"Customer {i+1}\" ,i+1 ,shortestPath_lst[i])\n",
        "\n",
        "  #output looping\n",
        "  i = 0 #counter for locate index\n",
        "  for name,origin,destination,distance in actualDistance_info_list:\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(f\"\"\"{name}\\nOrigin: {origin}\\nDestination: {destination}\\nDistance (Without going through Hub): {distance/1000.0} km\\n\"\"\")\n",
        "    for j in range(len(hub_totalDistance_list[i])):\n",
        "      print(f\"Travel through {courier_dict[j]['name']} ({courier_dict[j]['location']}) : {hub_totalDistance_list[i][j]} km\")\n",
        "    print()\n",
        "    print(f\"The least distance from origin to destination is through {courier_dict[shortestPath_lst[i]]['name']} ({courier_dict[shortestPath_lst[i]]['location']}) using A* Search\\nShortest Distance: {hub_totalDistance_list[i][shortestPath_lst[i]]} km\\n\")\n",
        "    i+=1\n",
        "  print(\"-------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWxK8mtXRrMz"
      },
      "source": [
        "##Driver code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHYLGHvPadSd"
      },
      "outputs": [],
      "source": [
        "#Driver code\n",
        "getAddress(courier_dict)\n",
        "plotLocation(courier_dict,\"q1\")\n",
        "analyseShortestDistance(customer_ori_dict,customer_dest_dict, courier_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XORmyTcU8VCk"
      },
      "source": [
        "# Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csEId4UmSC0M"
      },
      "source": [
        "##Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bVLxAE49k83"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ0S74gv9p-4"
      },
      "outputs": [],
      "source": [
        "pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mC9TE7D9xQs"
      },
      "outputs": [],
      "source": [
        "pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3QZA4VrZJI2"
      },
      "outputs": [],
      "source": [
        "pip install PyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWSOlKXUSFlZ"
      },
      "source": [
        "## Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0H1kpy7BT2N"
      },
      "outputs": [],
      "source": [
        "#Constant\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "#All positive & negative word store in text file\n",
        "downloaded = drive.CreateFile({'id':\"1hrv7QgM_wjN_um-nfRheA_83Fly5ENl5\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('negative.txt') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"143VK4J-6QcynXefD0AGkFnJkTq-SXgdN\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('positive.txt') \n",
        "\n",
        "#article url for information extraction\n",
        "hubArticleUrl_tupList = [(\"City-link Express\",\n",
        "                         [\"https://www.thesundaily.my/gear-up/isuzu-lorries--city-link-s-preferred-choice-AK729310\",\n",
        "                         \"https://postandparcel.info/60042/news/city-link-use-social-media-to-engage-with-customers/\",\n",
        "                         \"https://www.thestar.com.my/business/business-news/2015/01/05/citylink-mulls-main-market-listing-in-three-years\"\n",
        "                        ]),\n",
        "                        \n",
        "                        (\"Pos Laju\",\n",
        "                         [\"https://www.thestar.com.my/opinion/letters/2019/04/17/delivery-by---poslaju-needs-to-be-improved/\",\n",
        "                         \"https://www.prnewswire.com/news-releases/pos-laju-recognized-by-frost--sullivan-for-dominating-the-delivery-service-market-in-malaysia-on-the-strength-of-its-vast-channel-network-301194852.html\",\n",
        "                         \"https://www.theedgemarkets.com/article/mco-pos-malaysia-says-parcel-volume-rises-other-businesses-affected\"\n",
        "                        ]),\n",
        "                         \n",
        "                         (\"GDEX\",\n",
        "                          [\"https://www.theedgemarkets.com/article/gdex-look-creating-industrial-reit-part-next-growth-phase\",\n",
        "                          \"https://themalaysianreserve.com/2021/02/18/gdex-sees-higher-volume-base-for-logistics/\",\n",
        "                          \"https://www.thestar.com.my/business/business-news/2020/04/04/logistics-weathering-through-the-mco\"\n",
        "                         ]),\n",
        "                        \n",
        "                         (\"J&T\",\n",
        "                          [\"https://www.malaymail.com/news/malaysia/2021/02/07/courier-company-jt-express-explains-staffs-violent-handling-of-parcels-caug/1947791\",\n",
        "                         \"https://www.thestar.com.my/news/nation/2021/02/07/courier-company-says-sorry-over-039violent-sorting-of-packages039\",\n",
        "                         \"https://hype.my/2021/212126/jt-express-courier-truck-has-only-a-few-items-to-deliver-following-bad-publicity/\"\n",
        "                         ]),\n",
        "                          \n",
        "                         (\"DHL\",\n",
        "                          [\"https://www.theedgemarkets.com/article/tech-digitalisation-way-forward-dhl-express\",\n",
        "                           \"https://www.theedgemarkets.com/article/special-report-rocky-road-ahead-logistics-operators-amid-pandemic\",\n",
        "                           \"https://www.suasnews.com/2020/12/dhl-express-malaysia-partners-aerodyne-group-on-drone-delivery-services/\"\n",
        "                         ])]\n",
        "\n",
        "                        \n",
        "hubArticleUrl_backup_tupList = [(\"City-link Express\",\n",
        "                         [\"https://postandparcel.info/73872/news/big-can-be-beautiful-carriers-learn-lessons-from-city-link-collapse/\"\n",
        "                         ]),\n",
        "                        \n",
        "                        (\"Pos Laju\",\n",
        "                         [\"https://technave.com/gadget/Pos-Laju-adopts-new-verification-system-now-has-contactless-deliveries-23093.html\"\n",
        "                        ]),\n",
        "                         \n",
        "                         (\"GDEX\",\n",
        "                          [\"https://postandparcel.info/20397/news/malaysian-express-firm-targets-strong-growth-under-new-management/\"\n",
        "                          ]),\n",
        "                        \n",
        "                         (\"J&T\",\n",
        "                          [\"https://postandparcel.info/135696/news/freight/jt-express-this-new-addition-to-our-fleet-will-help-our-e-commerce-players-meet-the-needs-of-consumers/\"\n",
        "                          ]),\n",
        "                          \n",
        "                         (\"DHL\",\n",
        "                          [\"https://postandparcel.info/138277/news/e-commerce/dhl-supply-chain-to-take-on-up-to-2000-robots-by-2022/\"\n",
        "                         ])]                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S18Po2zhSIvl"
      },
      "source": [
        "##Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cgF9RbGBjlf"
      },
      "outputs": [],
      "source": [
        "#Algorithm \n",
        "\n",
        "# Python program for KMP Algorithm\n",
        "#Preprocessing\n",
        "def computeLPSArray(pat, M, lps):\n",
        "    len = 0 # length of the previous longest prefix suffix\n",
        "  \n",
        "    lps[0] # lps[0] is always 0\n",
        "    i = 1\n",
        "  \n",
        "    # the loop calculates lps[i] for i = 1 to M-1\n",
        "    while i < M:\n",
        "        if pat[i]== pat[len]:\n",
        "            len += 1\n",
        "            lps[i] = len\n",
        "            i += 1\n",
        "        else:\n",
        "            # This is tricky. Consider the example.\n",
        "            # AAACAAAA and i = 7. The idea is similar \n",
        "            # to search step.\n",
        "            if len != 0:\n",
        "                len = lps[len-1]\n",
        "  \n",
        "                # Also, note that we do not increment i here\n",
        "            else:\n",
        "                lps[i] = 0\n",
        "                i += 1\n",
        "\n",
        "def KMPSearch(pat, txt):\n",
        "    M = len(pat)\n",
        "    N = len(txt)\n",
        "\n",
        "    #Check Length, if they isnt same, terminate function immediately\n",
        "    if(M!=N):\n",
        "      return False\n",
        "\n",
        "    # create lps[] that will hold the longest prefix suffix \n",
        "    # values for pattern\n",
        "    lps = [0]*M\n",
        "    j = 0 # index for pat[]\n",
        "  \n",
        "    # Preprocess the pattern (calculate lps[] array)\n",
        "    computeLPSArray(pat, M, lps)\n",
        "    #Boolean flag to check whether matched or not\n",
        "    isFound = False\n",
        "    i = 0 # index for txt[]\n",
        "    while i < N:\n",
        "        if pat[j] == txt[i]:\n",
        "            i += 1\n",
        "            j += 1\n",
        "  \n",
        "        if j == M:\n",
        "            j = lps[j-1]\n",
        "            isFound = True\n",
        "\n",
        "        # mismatch after j matchesF\n",
        "        elif i < N and pat[j] != txt[i]:\n",
        "            # Do not match lps[0..lps[j-1]] characters,\n",
        "            # they will match anyway\n",
        "            if j != 0:\n",
        "                j = lps[j-1]\n",
        "            else:\n",
        "                i += 1\n",
        "  \n",
        "    return isFound\n",
        "\n",
        "##Comb Sort Algorithm     \n",
        "# This function used to find next gap from current\n",
        "def getNextGap(gap):\n",
        " \n",
        "    # Shrink gap by a Shrink factor 1.3\n",
        "    gap = (gap * 10)/13\n",
        "    if gap < 1:\n",
        "        return 1\n",
        "    return gap\n",
        " \n",
        "# Function to sort arr[] using Comb Sort\n",
        "def reverse_combSort(arr):\n",
        "    n = len(arr)\n",
        " \n",
        "    # Initialize gap\n",
        "    gap = n\n",
        " \n",
        "    # Initialize swapped as true to make sure that\n",
        "    # loop runs\n",
        "    swapped = True\n",
        " \n",
        "    # Keep running while gap is more than 1 and last\n",
        "    # iteration caused a swap\n",
        "    while gap !=1 or swapped == 1:\n",
        " \n",
        "        # Find next gap\n",
        "        gap = int(getNextGap(gap))\n",
        " \n",
        "        # Initialize swapped as false so that we can\n",
        "        # check if swap happened or not\n",
        "        swapped = False\n",
        " \n",
        "        # Compare all elements with current gap\n",
        "        for i in range(0, n-gap):\n",
        "            if arr[i] < arr[i + gap]:\n",
        "                arr[i], arr[i + gap]=arr[i + gap], arr[i]\n",
        "                swapped = True\n",
        "    return arr\n",
        "\n",
        "## Z algorithm  for pattern searching\n",
        "  \n",
        "# Fills Z array for given string str[]\n",
        "def getZarr(string, z):\n",
        "    n = len(string)\n",
        "  \n",
        "    # [L,R] make a window which matches\n",
        "    # with prefix of s\n",
        "    l, r, k = 0, 0, 0\n",
        "    for i in range(1, n):\n",
        "  \n",
        "        # if i>R nothing matches so we will calculate.\n",
        "        # Z[i] using naive way.\n",
        "        if i > r:\n",
        "            l, r = i, i\n",
        "  \n",
        "            # R-L = 0 in starting, so it will start\n",
        "            # checking from 0'th index. For example,\n",
        "            # for \"ababab\" and i = 1, the value of R\n",
        "            # remains 0 and Z[i] becomes 0. For string\n",
        "            # \"aaaaaa\" and i = 1, Z[i] and R become 5\n",
        "            while r < n and string[r - l] == string[r]:\n",
        "                r += 1\n",
        "            z[i] = r - l\n",
        "            r -= 1\n",
        "        else:\n",
        "  \n",
        "            # k = i-L so k corresponds to number which\n",
        "            # matches in [L,R] interval.\n",
        "            k = i - l\n",
        "  \n",
        "            # if Z[k] is less than remaining interval\n",
        "            # then Z[i] will be equal to Z[k].\n",
        "            # For example, str = \"ababab\", i = 3, R = 5\n",
        "            # and L = 2\n",
        "            if z[k] < r - i + 1:\n",
        "                z[i] = z[k]\n",
        "  \n",
        "            # For example str = \"aaaaaa\" and i = 2, \n",
        "            # R is 5, L is 0\n",
        "            else:\n",
        "  \n",
        "                # else start from R and check manually\n",
        "                l = i\n",
        "                while r < n and string[r - l] == string[r]:\n",
        "                    r += 1\n",
        "                z[i] = r - l\n",
        "                r -= 1\n",
        "  \n",
        "#check whether the pattern match the text using Z algo\n",
        "#Return True if pattern==text, else False\n",
        "def zcheck(pattern, text):\n",
        "  \n",
        "    # Create concatenated string \"P$T\"\n",
        "    concat = pattern + \"$\" + text\n",
        "    l = len(concat)\n",
        "  \n",
        "    # Construct Z array\n",
        "    z = [0] * l\n",
        "    getZarr(concat, z)\n",
        "  \n",
        "    # now looping through Z array for matching condition\n",
        "    for i in range(l):\n",
        "  \n",
        "        # if Z[i] (matched region) is equal to pattern\n",
        "        # length we got the pattern\n",
        "        if z[i] == len(pattern):\n",
        "          #If length of both pattern and text is same, matched!\n",
        "          if len(pattern) == len(text):\n",
        "            return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMjUU4agQ7ZL"
      },
      "source": [
        "## Functions Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrrzaKbCBq2v"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def removeStopword(text_lst):\n",
        "  \"\"\"\n",
        "  Remove stopword using built-in stopword from nltk library \n",
        "  Comparison done using z algorithm\n",
        "  Limitation: ONLY ENGLISH WORD\n",
        "  \"\"\"\n",
        "  clean_lst = text_lst\n",
        "  stopword_lst = stopwords.words('english')\n",
        "  for s in stopword_lst:\n",
        "    for t in clean_lst:\n",
        "      if zcheck(s,t):\n",
        "        clean_lst.remove(t)\n",
        "  return clean_lst\n",
        "\n",
        "def countWord_tupList(textList):\n",
        "  \"\"\"\n",
        "  Count occurance of each word in the list\n",
        "  return tuple list in format of (count, word)\n",
        "  \"\"\"\n",
        "  wordList = removeDuplicate(textList)\n",
        "  return [(textList.count(a) , a) for a in wordList]\n",
        "\n",
        "def removeNoise(text_lst):\n",
        "  \"\"\"\n",
        "  remove non usable characters such as punctuation, single-character digit, empty string \n",
        "  \"\"\"\n",
        "  #lower all text\n",
        "  text = \" \".join(text_lst)\n",
        "  text = text.lower()\n",
        "  \n",
        "  #remove punctuations\n",
        "  translatable =str.maketrans(\"\" , \"\", string.punctuation+\"“—”\") #need to add other font type of \"\"\n",
        "  text = text.translate(translatable)  \n",
        "\n",
        "  #Split into list element\n",
        "  text_list = text.split(\" \")\n",
        "  \n",
        "  #remove empty string & digits & single alphabet character\n",
        "  text_list = [substring.lower() for substring in text_lst if substring != \"\" and not substring.isdigit() and len(substring)>1]\n",
        "\n",
        "  #return list\n",
        "  return text_list\n",
        "\n",
        "def removeDuplicate(dup_list):\n",
        "  \"\"\"\n",
        "  remove duplicate in the list \n",
        "  \"\"\"\n",
        "  return list(set(dup_list))\n",
        "\n",
        "def plotWordCount(wordCountTuple_lst , article_title, barChartKeyword, outputName):\n",
        "  \"\"\"\n",
        "  Plot word count using Plotly Express \n",
        "  Title: Bar Chart of {barChartKeyword} for Article \"{article_title}\"\n",
        "  \"\"\"\n",
        "  if len(wordCountTuple_lst) == 0:\n",
        "    count, word = [], []\n",
        "  else:\n",
        "    count, word = zip(*wordCountTuple_lst)\n",
        "\n",
        "  fig = px.bar(x= word, y=count, labels = dict(x='Word', y = 'Count') ,title=f\"Bar Chart of {barChartKeyword} for Article \\\"{article_title}\\\"\" )\n",
        "  fig.show()\n",
        "  #pathName = f\"{outputName}.html\"\n",
        "  #fig.write_html(pathName)\n",
        "  #print(f\"Please find your bar chart at {pathName}\")\n",
        "\n",
        "def getFileList(filename):\n",
        "  \"\"\"\n",
        "  Read text file from downloaded file in Colab environment\n",
        "  Preprocess all word into lowercase and remove empty space before usage\n",
        "  \"\"\"\n",
        "  f= open(f\"{filename}.txt\",\"r\")\n",
        "  file_textList = f.read().split(\",\")\n",
        "  file_textList = [text.strip().lower() for text in file_textList]\n",
        "\n",
        "  return file_textList\n",
        "\n",
        "def exist(word, lst):\n",
        "  \"\"\"\n",
        "  Check existance of word using KMPsearch\n",
        "  Return boolean value (True if equal, else False)\n",
        "  \"\"\"\n",
        "  for item in lst:\n",
        "    if KMPSearch(item,word):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def separateWordType(positiveWord_list, negativeWord_list,textList):\n",
        "  \"\"\"\n",
        "  Separate word by its word type (positive, negative, neutral)\n",
        "  This function using exist function which use KMP to perform string matching checking\n",
        "  \"\"\"\n",
        "  positiveList , negativeList, neutralList = [], [], []\n",
        "  for word in textList:\n",
        "    if exist(word,positiveWord_list):\n",
        "      positiveList.append(word)\n",
        "    elif exist(word,negativeWord_list):\n",
        "      negativeList.append(word)\n",
        "    else:\n",
        "      neutralList.append(word)     \n",
        "\n",
        "  #Return 3 list\n",
        "  return positiveList, negativeList, neutralList\n",
        "\n",
        "def getSentimentScore(positiveList, negativeList, neutralList ,totalList):\n",
        "  \"\"\"\n",
        "  Get score of each word type by dividing the occurance with total word\n",
        "  return 3 float values\n",
        "  \"\"\"\n",
        "  if len(positiveList) ==0:\n",
        "    positiveScore = 0\n",
        "    negativeScore = len(negativeList)/len(totalList)\n",
        "  elif len(negativeList) ==0:\n",
        "    negativeScore = 0\n",
        "    positiveScore = len(positiveList)/len(totalList)\n",
        "  else :\n",
        "    positiveScore = len(positiveList)/len(totalList)\n",
        "    negativeScore = len(negativeList)/len(totalList)\n",
        "\n",
        "  neutralScore = len(neutralList)/len(totalList)\n",
        "\n",
        "  return positiveScore, negativeScore, neutralScore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVPgO3zsQ-21"
      },
      "source": [
        "## Functions Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKYvLCHJJLMz"
      },
      "outputs": [],
      "source": [
        "from newspaper import Article\n",
        "import nltk \n",
        "sentimentScore_list =[]\n",
        "\n",
        "def analyseSentiment():\n",
        "  global sentimentScore_list\n",
        "  sentimentScore_list =[]\n",
        "  #extract word from text file to get positive word from article later\n",
        "  positiveWord_list, negativeWord_list = getFileList(\"positive\"), getFileList(\"negative\")\n",
        "  print(\"------------------------------------------------------------------\")\n",
        "\n",
        "  for courier, url_list in hubArticleUrl_tupList:\n",
        "    courier_name = courier\n",
        "    count = 0\n",
        "    overall_positiveScore, overall_negativeScore = 0, 0\n",
        "    positiveScore_list, negativeScore_list =[] ,[]\n",
        "    print(f\"{courier}:\")\n",
        "\n",
        "    for url in url_list:\n",
        "\n",
        "      #initialisation of article by passing url, download and parse to extract info\n",
        "      article = Article(url, language = 'en')\n",
        "      article.download()\n",
        "      article.parse()\n",
        "\n",
        "      #Store article title for usage of output later\n",
        "      article_title = article.title\n",
        "\n",
        "      #tokenise full text into text list\n",
        "      article_textList = nltk.tokenize.word_tokenize(article.text)\n",
        "      \n",
        "      #Remove stopword and noise from article text list\n",
        "      temp = removeNoise(article_textList)\n",
        "      clean_article_textList = removeStopword(temp)\n",
        "\n",
        "      #get overall word count of article\n",
        "      total_article_wordCount_tupList = reverse_combSort(countWord_tupList(clean_article_textList))\n",
        "\n",
        "      print(\"\\nWord count plotting:\")\n",
        "\n",
        "      #plot word count of article\n",
        "      plotWordCount(total_article_wordCount_tupList, article_title , \"Total Word Count\", f\"{courier_name}_A{count+1}_Overall\")\n",
        "\n",
        "      #get positive, negative, neutral word list separated from total word\n",
        "      positive_article_word_list, negative_article_word_list, neutral_article_word_list = separateWordType(positiveWord_list, negativeWord_list, clean_article_textList)\n",
        "\n",
        "      #get word count of article for positive, negative, neutral\n",
        "      positive_article_wordCount_tupList = reverse_combSort(countWord_tupList(positive_article_word_list))\n",
        "      negative_article_wordCount_tupList = reverse_combSort(countWord_tupList(negative_article_word_list))\n",
        "      neutral_article_wordCount_tupList = reverse_combSort(countWord_tupList(neutral_article_word_list))\n",
        "\n",
        "      #plot word count of article for positive, negative, neutral\n",
        "      \n",
        "      plotWordCount(positive_article_wordCount_tupList, article_title , \"Positive Word Count\", f\"{courier_name}_A{count+1}_Positive\")\n",
        "      plotWordCount(negative_article_wordCount_tupList, article_title , \"Negative Word Count\", f\"{courier_name}_A{count+1}_Negative\")\n",
        "      plotWordCount(neutral_article_wordCount_tupList, article_title , \"Neutral Word Count\", f\"{courier_name}_A{count+1}_Neutral\")\n",
        "\n",
        "      positiveScore, negativeScore, neutralScore = getSentimentScore(positive_article_word_list, negative_article_word_list, neutral_article_word_list, clean_article_textList)\n",
        "      \n",
        "      positiveScore_list.append(positiveScore)\n",
        "      negativeScore_list.append(negativeScore)\n",
        "\n",
        "      print(f\"\\nSentiment score for Article {count+1} which related to {courier_name}:\")\n",
        "      print(f\"Positivity Score: {round(positiveScore,4)}\")\n",
        "      print(f\"Negativity Score: {round(negativeScore,4)}\")\n",
        "      print(f\"Neutral Score: {round(neutralScore,4)}\")     \n",
        "      \n",
        "      if positiveScore > negativeScore:\n",
        "        print(\"This article is giving positive sentiment\")\n",
        "      elif positiveScore < negativeScore :\n",
        "        print(\"This article is giving negative sentiment\")\n",
        "      else:\n",
        "        print(\"This article is giving neutral sentiment\")\n",
        "\n",
        "      print(\"------------------------------------------------------------------\")\n",
        "      overall_positiveScore+=positiveScore\n",
        "      overall_negativeScore+=negativeScore\n",
        "      count+=1\n",
        "    \n",
        "    overall_positiveScore/=len(url_list)\n",
        "    overall_negativeScore/=len(url_list)\n",
        "    overall_Score = overall_positiveScore - overall_negativeScore\n",
        "    \n",
        "    sentimentScore_list.append((overall_Score, courier, [overall_positiveScore, overall_negativeScore]))\n",
        "\n",
        "  print(\"\\nOverall:\")\n",
        "\n",
        "  for overall_score, courier, pos_neg_score_list in sentimentScore_list:\n",
        "    print(f\"Scoring of {courier} (Score = Average Positive Score - Average Negative Score) : {round(overall_score,4)}\")\n",
        "\n",
        "  sentimentScore_list = reverse_combSort(sentimentScore_list)\n",
        "  print(f\"\\n{sentimentScore_list[0][1]} have the best sentiment.\")\n",
        "  print(f\"{sentimentScore_list[-1][1]} have the worst sentiment.\")\n",
        "  print(\"------------------------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfusR3dHQ20X"
      },
      "source": [
        "## Driver code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EggzJgpoDVS"
      },
      "outputs": [],
      "source": [
        "#Driver code\n",
        "analyseSentiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41sPSqgr8VbN"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HynBoUUaQsVn"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JiECf0jFZ6lG"
      },
      "outputs": [],
      "source": [
        "def calcMinRun(n):\n",
        "    \"\"\"Returns the minimum length of a\n",
        "    run from 23 - 64 so that\n",
        "    the len(array)/minrun is less than or\n",
        "    equal to a power of 2.\n",
        " \n",
        "    e.g. 1=>1, ..., 63=>63, 64=>32, 65=>33,\n",
        "    ..., 127=>64, 128=>32, ...\n",
        "    \"\"\"\n",
        "    MIN_MERGE = 16\n",
        "    r = 0\n",
        "    while n >= MIN_MERGE:\n",
        "        r |= n & 1\n",
        "        n >>= 1\n",
        "    return n + r\n",
        " \n",
        " \n",
        "# This function sorts array from left index to\n",
        "# to right index which is of size atmost RUN\n",
        "def insertionSort(arr, left, right):\n",
        "    for i in range(left + 1, right + 1):\n",
        "        j = i\n",
        "        while j > left and arr[j] < arr[j - 1]:\n",
        "            arr[j], arr[j - 1] = arr[j - 1], arr[j]\n",
        "            j -= 1\n",
        " \n",
        " \n",
        "# Merge function merges the sorted runs\n",
        "def merge(arr, l, m, r):\n",
        "     \n",
        "    # original array is broken in two parts\n",
        "    # left and right array\n",
        "    len1, len2 = m - l + 1, r - m\n",
        "    left, right = [], []\n",
        "    for i in range(0, len1):\n",
        "        left.append(arr[l + i])\n",
        "    for i in range(0, len2):\n",
        "        right.append(arr[m + 1 + i])\n",
        " \n",
        "    i, j, k = 0, 0, l\n",
        "     \n",
        "    # after comparing, we merge those two array\n",
        "    # in larger sub array\n",
        "    while i < len1 and j < len2:\n",
        "        if left[i] <= right[j]:\n",
        "            arr[k] = left[i]\n",
        "            i += 1\n",
        " \n",
        "        else:\n",
        "            arr[k] = right[j]\n",
        "            j += 1\n",
        " \n",
        "        k += 1\n",
        " \n",
        "    # Copy remaining elements of left, if any\n",
        "    while i < len1:\n",
        "        arr[k] = left[i]\n",
        "        k += 1\n",
        "        i += 1\n",
        " \n",
        "    # Copy remaining element of right, if any\n",
        "    while j < len2:\n",
        "        arr[k] = right[j]\n",
        "        k += 1\n",
        "        j += 1\n",
        " \n",
        " \n",
        "# Iterative Timsort function to sort the\n",
        "# array[0...n-1] (similar to merge sort)\n",
        "def timSort(arr):\n",
        "    n = len(arr)\n",
        "    minRun = calcMinRun(n)\n",
        "     \n",
        "    # Sort individual subarrays of size RUN\n",
        "    for start in range(0, n, minRun):\n",
        "        end = min(start + minRun - 1, n - 1)\n",
        "        insertionSort(arr, start, end)\n",
        " \n",
        "    # Start merging from size RUN (or 32). It will merge\n",
        "    # to form size 64, then 128, 256 and so on ....\n",
        "    size = minRun\n",
        "    while size < n:\n",
        "         \n",
        "        # Pick starting point of left sub array. We\n",
        "        # are going to merge arr[left..left+size-1]\n",
        "        # and arr[left+size, left+2*size-1]\n",
        "        # After every merge, we increase left by 2*size\n",
        "        for left in range(0, n, 2 * size):\n",
        " \n",
        "            # Find ending point of left sub array\n",
        "            # mid+1 is starting point of right sub array\n",
        "            mid = min(n - 1, left + size - 1)\n",
        "            right = min((left + 2 * size - 1), (n - 1))\n",
        " \n",
        "            # Merge sub array arr[left.....mid] &\n",
        "            # arr[mid+1....right]\n",
        "            if mid < right:\n",
        "                merge(arr, left, mid, right)\n",
        " \n",
        "        size = 2 * size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2kH-u3SQyHx"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7HmVFNXqaBy1"
      },
      "outputs": [],
      "source": [
        "def reconstructList(sentimentScore_list, courier_dict):\n",
        "  \"\"\"\n",
        "  Reconsturct list back to default arrangement as constant\n",
        "  \"\"\"\n",
        "  constructed_list =[]\n",
        "  for i in range(len(courier_dict)):\n",
        "    name = courier_dict[i]['name']\n",
        "    for j in range(len(sentimentScore_list)):\n",
        "      if sentimentScore_list[j][1]== name:\n",
        "        constructed_list.append(sentimentScore_list[j])\n",
        "  return constructed_list\n",
        " \n",
        "def getOverallScore(sentimentScore_list):\n",
        "  \"\"\"\n",
        "  Extract overall score from the sentiment score list\n",
        "  \"\"\"\n",
        "  overallScore_list=[]\n",
        "  for lst in sentimentScore_list:\n",
        "    overallScore_list.append(lst[0])\n",
        "  return overallScore_list\n",
        " \n",
        "def minMaxNorm(mini, value, maxi , weightage):\n",
        "  \"\"\"\n",
        "  Perform min max normalisation calculation\n",
        "  \"\"\"\n",
        "  return round((((value-mini)/(maxi-mini)) * weightage),3)\n",
        " \n",
        "def displaySentimentScore(sentimentScore_list):\n",
        "  \"\"\"\n",
        "  Display courier company sentimentals\n",
        "  \"\"\"\n",
        "  for overall_score, courier_name , scoring in sentimentScore_list:\n",
        "    print(f\"{courier_name}\\nPositivity: {round(scoring[0],4)}\\nNegativity: {round(scoring[1],4)}\\n\")\n",
        " \n",
        "def reconstruct_hubDistance_tupList(totalDistance_list , courierDict):\n",
        "  \"\"\"\n",
        "  return sorted tupleList in form of \n",
        "  [[(distance1, hub1_name, hub1_location),(distance2, hub2_name, hub2_location),...],...]\n",
        "  \"\"\"\n",
        "  tupList=[]\n",
        "  for i in range(len(totalDistance_list)):\n",
        "    tempList=[]\n",
        "    for j in range(len(totalDistance_list[i])):\n",
        "      tempList.append((totalDistance_list[i][j], courierDict[j]['name'], courierDict[j]['location']))\n",
        "    tupList.append(tempList)\n",
        "  return tupList\n",
        " \n",
        "def get_reverseSorted_totalScore_tupList(hub_totalDistance_list,reconstructed_hubDistance_tupList,minMaxScore_list,distance_weightage):\n",
        "  \"\"\"\n",
        "  Does calculation of overall ranking score based on raw distance and sentiments\n",
        "  \"\"\"\n",
        "  tupList = []\n",
        "  for i in range(len(reconstructed_hubDistance_tupList)):\n",
        " \n",
        "    tempList=[]  #temporary list for storing of data\n",
        "    minDistance = min(hub_totalDistance_list[i]) #get minimum distance from all distance\n",
        "    maxDistance = max(hub_totalDistance_list[i]) #get maximum distance form all distance\n",
        " \n",
        "    for j in range(len(reconstructed_hubDistance_tupList[i])):\n",
        " \n",
        "      distance = reconstructed_hubDistance_tupList[i][j][0]\n",
        "      minMaxDistance_score = -(minMaxNorm(minDistance, distance, maxDistance, distance_weightage))\n",
        "      \n",
        "      #round to 3 d.p\n",
        "      totalScore = round(minMaxScore_list[j]+(distance_weightage+minMaxDistance_score), 3) \n",
        "      tempList.append((totalScore,distance,reconstructed_hubDistance_tupList[i][j][1], reconstructed_hubDistance_tupList[i][j][2] ))\n",
        " \n",
        "    #sorting of the list\n",
        "    timSort(tempList)\n",
        "    #reverse the arrangement as we want from best score (highest) to worst score (lowest)\n",
        "    tempList = sorted(tempList, reverse= True)\n",
        "    tupList.append(tempList)\n",
        " \n",
        "  return tupList\n",
        " \n",
        "def extract_ori_dest(customer_ori_dict,customer_dest_dict):\n",
        "  \"\"\"\n",
        "  Extract information from customer ori_dict and dest_dict and\n",
        "  return tuple list which contain \n",
        "  i) Customer name \n",
        "  ii) Customer origin location\n",
        "  iii) Customer destination location\n",
        "  \"\"\"\n",
        "  tupList = []\n",
        "  for i in range(1,len(customer_ori_dict)+1):\n",
        "    tempTup=(customer_ori_dict[i]['name'], customer_ori_dict[i]['location'], customer_dest_dict[i]['location'])\n",
        "    tupList.append(tempTup)\n",
        "  return tupList\n",
        " \n",
        "def analyseRanking(courier_dict,hub_totalDistance_list, sentimentScore_list, customer_ori_dict, customer_dest_dict):\n",
        "  \"\"\"\n",
        "  Main start point of the program\n",
        "  \"\"\"\n",
        " \n",
        "  # Weight for sentiment and distance is 1:1 so each give 50 (can be manipulated)\n",
        "  sentimentScore_weightage = 50\n",
        "  distance_weightage  = 50\n",
        " \n",
        "  sentimentScore_list = reconstructList(sentimentScore_list,courier_dict)\n",
        "  overall_sentimentScore_list = getOverallScore(sentimentScore_list)\n",
        "  minScore = min(overall_sentimentScore_list)\n",
        "  maxScore = max(overall_sentimentScore_list)\n",
        "  minMaxScore_list = [minMaxNorm(minScore, value, maxScore, sentimentScore_weightage) for value in overall_sentimentScore_list]\n",
        " \n",
        "  reconstructed_hubDistance_tupList = reconstruct_hubDistance_tupList(hub_totalDistance_list, courier_dict)\n",
        "  reverse_totalScore_list = get_reverseSorted_totalScore_tupList(hub_totalDistance_list,reconstructed_hubDistance_tupList,minMaxScore_list, distance_weightage)\n",
        " \n",
        "  #Format output into structured form\n",
        " \n",
        "  print('-----------------------------------------------------------------')\n",
        "  print(\"Courier Sentiment Score:\\n\")\n",
        "  displaySentimentScore(sentimentScore_list) #display each courier positivity & negativity\n",
        "  print('-----------------------------------------------------------------')\n",
        " \n",
        " \n",
        "  customer_info_tupList = extract_ori_dest(customer_ori_dict, customer_dest_dict)\n",
        "  for i in range(len(customer_info_tupList)):\n",
        "    print('-----------------------------------------------------------------')\n",
        "    print(f\"{customer_info_tupList[i][0]}\\nOrigin: {customer_info_tupList[i][1]}\\nDestination: {customer_info_tupList[i][2]}\\n\")\n",
        " \n",
        "    print(\"Distance calculated:\")\n",
        "    for j in range(len(reverse_totalScore_list[i])):\n",
        "      print(f\"Travel through Hub {reverse_totalScore_list[i][j][2]} at {reverse_totalScore_list[i][j][3]} : {reverse_totalScore_list[i][j][1]} km \")\n",
        " \n",
        "    print(\"\\nCourier Ranking:\")\n",
        "    for j in range(len(reverse_totalScore_list[i])):\n",
        "      print(f\"{reverse_totalScore_list[i][j][2]} : {reverse_totalScore_list[i][j][0]}\")\n",
        " \n",
        "    print(f\"\\nRecommended courier company is {reverse_totalScore_list[i][0][2]} \")\n",
        "  print('-----------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4u_fAUZQmEG"
      },
      "source": [
        "## Driver code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PncZ5_Y_aMO5"
      },
      "outputs": [],
      "source": [
        "analyseRanking(courier_dict,hub_totalDistance_list,sentimentScore_list, customer_ori_dict, customer_dest_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9khhxc5e8ViW"
      },
      "source": [
        "# Problem 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGim0tpiBEq2"
      },
      "source": [
        "## **DTW Implementation from scratch**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDSN8MjYch3n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " \n",
        "# Find the minimum value \n",
        "def find_min(*args):\n",
        "  minimum_value = min(args)\n",
        "  minimum_value_index = args.index(minimum_value)\n",
        "  return minimum_value_index\n",
        " \n",
        " \n",
        "# Calculate the cost without accumulation in the matrix\n",
        "# Note: the formula for the np.linalg.norm function is max(sum(abs(x), axis=0))\n",
        "def calc_init_cost(x, y):\n",
        "    result = [y[0]-x[0], y[1] - x[1]]\n",
        "    return np.linalg.norm(result, 1)\n",
        " \n",
        " \n",
        "# ts_1, ts_2 is the transposed mfcc input, dist is the distance method used.\n",
        "def get_dtw(ts_1,ts_2):\n",
        " \n",
        "    # Get the number of row and column of the matrix.\n",
        "    row, col = len(ts_1), len(ts_2)\n",
        "    # Initiase a 2D numpy array filled with zeros with addition of 1 row and 1 column.\n",
        "    mod_cost_matrix = np.zeros((row+1, col + 1))\n",
        "    # Make the first row and first column as infinity to prevent out of boundary error \n",
        "    # when finding the value in the accumulated cost matrix.\n",
        "    mod_cost_matrix[0, 1:] = np.inf\n",
        "    mod_cost_matrix[1:, 0] = np.inf\n",
        " \n",
        "    # Assign the matrix without the infinity row and column to a variable \n",
        "    cost_matrix = mod_cost_matrix[1:, 1:]\n",
        " \n",
        "    # Calculate the cost in the matrix by using the coordinate of two time series.\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            cost_matrix[i, j] = calc_init_cost(ts_1[i], ts_2[j])\n",
        " \n",
        "    # Copy the cost matrix into a variable before doing accumulated calculation.\n",
        "    copied_cost_matrix = cost_matrix.copy()\n",
        " \n",
        "    # Find out the accumulated value in the cost matrix\n",
        "    for i in range(row):\n",
        "        for j in range(col):\n",
        "            cost_matrix[i, j] += min(mod_cost_matrix[i, j], mod_cost_matrix[i, j+1], mod_cost_matrix[i+1, j])\n",
        " \n",
        "    # Check whether the length of the mfcc array is 1 as the path is just a straight horizontal / verticle line.\n",
        "    if len(ts_1) == 1:\n",
        "        min_path = np.zeros(len(ts_2)), range(len(ts_2))\n",
        "    elif len(ts_2) == 1:\n",
        "        min_path = range(len(ts_1)), np.zeros(len(ts_1))\n",
        "    else:\n",
        "        min_path = _traceback(cost_matrix)\n",
        " \n",
        "    return cost_matrix[-1, -1] / sum(cost_matrix.shape), copied_cost_matrix, cost_matrix, min_path\n",
        " \n",
        " \n",
        "# Trace the path from the end of the accumulated cost matrix.\n",
        "def _traceback(acc_mat):\n",
        "    # Get the upper most right value in the matrix\n",
        "    x, y = np.array(acc_mat.shape) - 1 # 2 here is taken account of 0-indexing.\n",
        "    p, q = [x], [y]\n",
        "    # Track back until reaching bottom most left value in the matrix\n",
        "    while (x > 0) or (y > 0):\n",
        "        # Check whether the path is at the most left or the most bottom. \n",
        "        if y == 0:\n",
        "            index = 1\n",
        "        elif x == 0:\n",
        "            index = 2\n",
        "        else:\n",
        "            index = find_min(acc_mat[x-1, y-1], acc_mat[x-1, y], acc_mat[x, y-1])\n",
        " \n",
        "        # Update the coordinate.\n",
        "        if index == 0:\n",
        "            x -= 1\n",
        "            y -= 1\n",
        "        elif index == 1:\n",
        "            x -= 1\n",
        "        else:\n",
        "            y -= 1\n",
        "        \n",
        "        # Insert the coordinate to the array.\n",
        "        p.insert(0, x)\n",
        "        q.insert(0, y)\n",
        " \n",
        "    return np.array(p), np.array(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndrmGVT-BLAx"
      },
      "source": [
        "## (Using Own DTW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGmi75MIdlal"
      },
      "outputs": [],
      "source": [
        "pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkCU24riBO0W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import IPython.display\n",
        "import librosa\n",
        "import librosa.display\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXb5TXGkBRWm"
      },
      "outputs": [],
      "source": [
        "# Display audio to play in output\n",
        "def display(basepath, audio_file):\n",
        "\n",
        "  # Obtain the audio path\n",
        "  audio_path = os.path.join(basepath, audio_file)\n",
        "\n",
        "  # Check whether the file is existing.\n",
        "  if not os.path.isfile(audio_path):\n",
        "    print(\"No such {} file in the input folder.\".format(audio_file))\n",
        "    return\n",
        "\n",
        "  # Display Audio\n",
        "  return IPython.display.Audio(audio_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucj2YjRTBS5i"
      },
      "outputs": [],
      "source": [
        "# Split the audio test file into several chunk\n",
        "# min_silence_len is the minimum time for the silence period.\n",
        "# silence thresh is the threshould decibal value for silence period.\n",
        "# These two variables are used to assist in spliting the audio track.\n",
        "# Input path should be the basepath for the raw audio\n",
        "# Output path should be the basepath for the processed audio\n",
        "def audio_split(input_path, output_path, audio_test_file, min_silence_len = 500, silence_thresh = -50):\n",
        "\n",
        "  # Path joining\n",
        "  audio_test_file_path = os.path.join(input_path, audio_test_file)\n",
        "\n",
        "  # Empty the process folder if contains any audio files\n",
        "  if len(os.listdir(output_path)) > 0:\n",
        "    print(\"DELETING EXISTING FILE!\")\n",
        "    for f in os.listdir(output_path):\n",
        "      os.remove(os.path.join(output_path, f))\n",
        "\n",
        "  # Reading the audio\n",
        "  audio = AudioSegment.from_file(audio_test_file_path)\n",
        "\n",
        "  # Split track \n",
        "  audio_chunk_list = split_on_silence(\n",
        "      audio,\n",
        "      min_silence_len = min_silence_len,\n",
        "      silence_thresh = silence_thresh\n",
        "  )\n",
        "\n",
        "  # Save the audio chunk into process folder\n",
        "  for index, audio_chunk in enumerate(audio_chunk_list):\n",
        "    audio_chunked_file_path = os.path.join(output_path, \"{}.wav\".format(index))\n",
        "    audio_chunk.export(audio_chunked_file_path, format = \"wav\")\n",
        "\n",
        "  print(\"Split Process completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcpJlTDGBTX3"
      },
      "outputs": [],
      "source": [
        "# Extract mfcc from the chunk_wav file\n",
        "# input_path should be the basepath for the processed audio\n",
        "def extract_mfcc(input_path):\n",
        "\n",
        "  # Initialise an empty list\n",
        "  mfcc_list = []\n",
        "\n",
        "  # Getting the number of file in process\n",
        "  no_file = len(os.listdir(input_path))\n",
        "\n",
        "  if no_file == 0:\n",
        "    print(\"Please pre-processed your audio file. Thanks!\")\n",
        "    return\n",
        "\n",
        "  # mfcc extraction\n",
        "  for index in range(no_file):\n",
        "    audio_chunk_path = os.path.join(input_path, \"{}.wav\".format(index))\n",
        "    x, sr = librosa.load(audio_chunk_path)\n",
        "    mfcc = librosa.feature.mfcc(x, sr).T\n",
        "    mfcc_list.append(mfcc)\n",
        "\n",
        "  return mfcc_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFAfuctLBU3R"
      },
      "outputs": [],
      "source": [
        "# Word analysis in speech (Ensure that the file type is wav.)\n",
        "# Threshold value helps to check the word similarity.\n",
        "# Input path should be the basepath for the tested audio file.\n",
        "def analysis(input_path, file_name, word, mfcc_list, threshold = 60):\n",
        "\n",
        "  # Check whether have the input file.\n",
        "  if not os.path.isfile(os.path.join(input_path, file_name)):\n",
        "    print(\"No such {} file in the input folder.\".format(file_name))\n",
        "    return\n",
        "\n",
        "  # Extract the mfcc from the input file.\n",
        "  x, sr = librosa.load(os.path.join(input_path, file_name))\n",
        "  input_mfcc = librosa.feature.mfcc(x, sr).T\n",
        "  len_input_mfcc = len(input_mfcc)\n",
        "\n",
        "  # Initialised the important variable.\n",
        "  file_number = 0\n",
        "  min_distance = np.inf\n",
        "\n",
        "  # Speech analysis using DTW algorithm to find the smallest distance using the chunked audio.\n",
        "  for index, mfcc in enumerate(mfcc_list):\n",
        "    # Get the length of the mfcc.\n",
        "    len_mfcc = len(mfcc)\n",
        "\n",
        "    # if length of the input greater than the chunk.\n",
        "    if len_input_mfcc > len_mfcc:\n",
        "      dist, cost, acc_cost, path = get_dtw(input_mfcc, mfcc)\n",
        "      if dist < min_distance:\n",
        "        file_number = index\n",
        "        min_distance = dist\n",
        "    \n",
        "    else:\n",
        "      # Split the audio based on the input size.\n",
        "      for start in range(len_input_mfcc, len_mfcc, 10):\n",
        "        split_mfcc = mfcc[start - len_input_mfcc: start, :]\n",
        "        dist, cost, acc_cost, path = get_dtw(input_mfcc, split_mfcc)\n",
        "        if dist < min_distance:\n",
        "          file_number = index\n",
        "          min_distance = dist\n",
        "\n",
        "  # Check the word similarity\n",
        "  if min_distance > threshold:\n",
        "    print(\"The word {} is not found in the speech. The minimum distance found is {}.\".format(word, round(min_distance, 4)))\n",
        "  else:\n",
        "    print(\"The word {} is found in the speech. The minimum distance found is {}.\".format(word, round(min_distance, 4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkUhnl3ZBWeM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mounting the drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PjUK7IzBXw3"
      },
      "outputs": [],
      "source": [
        "# Set up the basepath for the audio file\n",
        "\n",
        "RAWBASEPATH = \"/content/drive/MyDrive/WIA2005 Group 9 Assignment/Problem 4/dtw/raw\" # The basepath for raw audio input\n",
        "PROCESSEDBASEPATH = \"/content/drive/MyDrive/WIA2005 Group 9 Assignment/Problem 4/dtw/process\" # The basepath for processed audio input\n",
        "WORDBASEPATH = \"/content/drive/MyDrive/WIA2005 Group 9 Assignment/Problem 4/dtw/word\" # The basepath for word audio input\n",
        "\n",
        "'''\n",
        "RAWBASEPATH = \"/content/drive/MyDrive/dtw/raw\" # The basepath for raw audio input\n",
        "PROCESSEDBASEPATH = \"/content/drive/MyDrive/dtw/process\" # The basepath for processed audio input\n",
        "WORDBASEPATH = \"/content/drive/MyDrive/dtw/word\" # The basepath for word audio input\n",
        "'''\n",
        "\n",
        "RAWAUDIO = \"test.mp4\" # File name for the raw audio\n",
        "WORDAUDIO = \"CocaCola_1.wav\" # File name for the word audio\n",
        "WORDAUDIO_2 = \"test 2.wav\"\n",
        "WORD = \"CocaCola\" # Word want to be detected\n",
        "WORD_2 =\"J&T Express\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KyAfZ6zUBZ36"
      },
      "outputs": [],
      "source": [
        "# Displaying raw audio\n",
        "display(RAWBASEPATH, RAWAUDIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4W7hvZXhBbXX"
      },
      "outputs": [],
      "source": [
        "# Displaying word audio\n",
        "display(WORDBASEPATH, WORDAUDIO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHTeiLRRBbuQ"
      },
      "outputs": [],
      "source": [
        "# Displaying word audio\n",
        "display(WORDBASEPATH, WORDAUDIO_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnevqbx9Bc-V"
      },
      "outputs": [],
      "source": [
        "# Output\n",
        "audio_split(RAWBASEPATH, PROCESSEDBASEPATH, RAWAUDIO, 100)\n",
        "mfcc_list = extract_mfcc(PROCESSEDBASEPATH)\n",
        "analysis(WORDBASEPATH, WORDAUDIO, WORD, mfcc_list, threshold = 70)\n",
        "analysis(WORDBASEPATH, WORDAUDIO_2, WORD_2, mfcc_list, threshold = 70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9khhxc5e8ViW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}